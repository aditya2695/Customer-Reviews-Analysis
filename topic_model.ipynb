{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adity\\.conda\\envs\\env_pytorch\\lib\\site-packages\\past\\builtins\\misc.py:45: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  from imp import reload\n"
     ]
    }
   ],
   "source": [
    "#data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "#file and system operations\n",
    "import os\n",
    "import sys\n",
    "assert sys.version_info >= (3,5)\n",
    "#visualization imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "#consistent sized plots\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize']=12,5\n",
    "rcParams['axes.labelsize']=12\n",
    "rcParams['ytick.labelsize']=12\n",
    "rcParams['xtick.labelsize']=12\n",
    "#handle unwanted warnings \n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "#view all the columns\n",
    "pd.options.display.max_columns = None\n",
    "#basic text manipulation libraries\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import gensim\n",
    "#plotting tools\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim #dont skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "      <th>tokens</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BA cancelled my flight home to Heathrow on Dec...</td>\n",
       "      <td>['cancelled', 'home', 'heathrow', 'dec', '19th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BA cancelled my flight home, the last flight o...</td>\n",
       "      <td>['cancelled', 'home', 'last', 'day', 'heathrow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Turned up 3.5 hours in advance, Terminal 5 at ...</td>\n",
       "      <td>['turned', '3.5', 'hours', 'advance', 'termina...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Boarding – at gate at LGW they called Group 1 ...</td>\n",
       "      <td>['boarding', '–', 'gate', 'lgw', 'called', 'gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Missing baggage customer service was the worst...</td>\n",
       "      <td>['missing', 'baggage', 'customer', 'service', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews  \\\n",
       "0  BA cancelled my flight home to Heathrow on Dec...   \n",
       "1  BA cancelled my flight home, the last flight o...   \n",
       "2  Turned up 3.5 hours in advance, Terminal 5 at ...   \n",
       "3  Boarding – at gate at LGW they called Group 1 ...   \n",
       "4  Missing baggage customer service was the worst...   \n",
       "\n",
       "                                              tokens  sentiment  \n",
       "0  ['cancelled', 'home', 'heathrow', 'dec', '19th...          0  \n",
       "1  ['cancelled', 'home', 'last', 'day', 'heathrow...          0  \n",
       "2  ['turned', '3.5', 'hours', 'advance', 'termina...          0  \n",
       "3  ['boarding', '–', 'gate', 'lgw', 'called', 'gr...          0  \n",
       "4  ['missing', 'baggage', 'customer', 'service', ...          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews = pd.read_csv('data/neg_reviews.csv')\n",
    "neg_reviews.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1488, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Write a function to proprocess the entire dataset \n",
    "'''\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    '''This function will lemmatize on Noun POS and stem the text'''\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text,pos='n'))\n",
    "    #return (WordNetLemmatizer().lemmatize(text,pos='n'))\n",
    "\n",
    "#tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    '''Function to break into word tokens, remove stopwords, remove short words and finally to lemmatize and stem the individual tokens'''\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['worst', 'camera', 'seen', 'configur', 'mobil', 'phone', 'better', 'camera', 'resolut', 'batteri', 'drain', 'faster']\n"
     ]
    }
   ],
   "source": [
    "#check for a sample review\n",
    "result = preprocess('The worst camera I have ever seen. Even my very old configuration mobile phone had a better camera resolution. Battery draining faster.')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = []\n",
    "\n",
    "#uncomment below line to find the topics for a particular sentiment\n",
    "#reviews = reviews[reviews['sentiment']==1]\n",
    "\n",
    "for doc in neg_reviews['reviews']:\n",
    "    processed_docs.append(preprocess(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cancel', 'flight', 'home', 'heathrow', 'face', 'sensibl', 'weather', 'iceland', 'appal', 'accept', 'flight', 'cancel', 'hour', 'time', 'time', 'struggl', 'dread', 'drive', 'condit', 'airport', 'near', 'condit', 'taken', 'advic', 'spare', 'need', 'travel', 'road', 'end', 'close', 'book', 'flight', 'downgrad', 'sin', 'have', 'check', 'today', 'type', 'cancel', 'flight', 'despit', 'road', 'keflavík', 'close', 'check', 'thing', 'help', 'offer'], ['cancel', 'flight', 'home', 'flight', 'heathrow', 'tri', 'push', 'flight', 'london', 'citi', 'hour', 'later', 'heathrow', 'want', 'cross', 'london', 'late', 'night', 'public', 'transport', 'luggag', 'especi', 'rail', 'strike', 'announc', 'time', 'book', 'flight', 'home', 'lufthansa', 'cost', 'economi', 'seat', 'home', 'heathrow', 'termin', 'friday', 'night', 'home', 'famili', 'news', 'refund', 'month', 'shame', 'fli', 'pleasur', 'experi', 'cancel', 'move', 'journey', 'book', 'year', 'regrett', 'airlin', 'better', 'reliabl', 'custom', 'servic'], ['turn', 'hour', 'advanc', 'termin', 'london', 'heathrow', 'carnag', 'peopl', 'tri', 'check', 'desk', 'man', 'long', 'queue', 'secur', 'line', 'open', 'peopl', 'stress', 'push', 'shout', 'flight', 'delay', 'hour', 'accord', 'pilot', 'delay', 'suitcas', 'need', 'load', 'miss', 'connect', 'flight', 'doha', 'hour', 'wait', 'receiv', 'voucher', 'approx', 'arriv', 'later', 'suitcas', 'arriv', 'file', 'claim', 'desk', 'day', 'updat', 'inform', 'offer', 'compens', 'solut', 'offer', 'spoilt', 'holiday', 'person', 'belong', 'gone', 'xmas', 'gift', 'miss', 'extens', 'expens', 'know', 'reimburs', 'bewar', 'expect', 'level', 'zero', 'custom', 'servic', 'plane', 'zero', 'help', 'go', 'wrong', 'miss', 'person', 'avoid', 'futur', 'cost'], ['board', 'gate', 'call', 'group', 'board', 'world', 'travel', 'plus', 'group', 'call', 'group', 'surg', 'forward', 'number', 'board', 'group', 'cabin', 'crew', 'treat', 'passeng', 'inconveni', 'move', 'plane', 'point', 'passeng', 'come', 'toilet', 'crew', 'member', 'approach', 'crew', 'member', 'push', 'door', 'close', 'push', 'toilet', 'went', 'past', 'word', 'acknowledg', 'stood', 'asid', 'past', 'servic', 'final', 'total', 'seat', 'lunch', 'meal', 'brought', 'peopl', 'serv', 'unusu', 'seat', 'came', 'vegetarian', 'option', 'left', 'menu', 'option', 'cabin', 'steward', 'said', 'given', 'meal', 'second', 'meal', 'serv', 'land', 'server', 'vegetarian', 'option', 'left', 'ask', 'possibl', 'commenc', 'serv', 'second', 'meal', 'choic', 'meal', 'said', 'hadn', 'thought', 'food', 'ined', 'vegetarian', 'cannelloni', 'pasta', 'uncook', 'bread', 'roll', 'crispi', 'like', 'toast', 'half', 'soft', 'half', 'thing', 'babi', 'meal', 'snack', 'consist', 'biscuit', 'pack', 'shortbread', 'pretti', 'basic', 'drink', 'wine', 'hour', 'hour', 'flight', 'toilet', 'toilet', 'access', 'direct', 'section', 'walk', 'halfway', 'plane', 'economi', 'reach', 'share', 'toilet', 'flew', 'virgin', 'atlant', 'month', 'equival', 'premium', 'economi', 'better', 'organis', 'board', 'sparkl', 'wine', 'orang', 'board', 'water', 'orang', 'food', 'option', 'avail', 'seat', 'better', 'food', 'proper', 'glass', 'cup', 'drink', 'dedic', 'toilet', 'access', 'help', 'snack', 'help', 'servic', 'cabin', 'crew', 'look', 'virgin', 'futur'], ['miss', 'baggag', 'custom', 'servic', 'worst', 'experienc', 'arriv', 'phoenix', 'sunday', 'even', 'tag', 'show', 'bag', 'arriv', 'monday', 'took', 'day', 'final', 'deliv', 'understood', 'delay', 'caus', 'freez', 'london', 'luggag', 'final', 'flight', 'fill', 'requir', 'form', 'phoenix', 'airport', 'attempt', 'onlin', 'receiv', 'messag', 'say', 'bag', 'report', 'miss', 'enter', 'track', 'code', 'sent', 'email', 'email', 'receiv', 'phone', 'call', 'daili', 'told', 'messag', 'sent', 'hear', 'hour', 'final', 'email', 'track', 'number', 'receiv', 'tuesday', 'night', 'went', 'onlin', 'track', 'number', 'wednesday', 'request', 'provid', 'inform', 'given', 'form', 'airport', 'luggag', 'final', 'deliv', 'thursday', 'afternoon'], ['british', 'airway', 'flag', 'carrier', 'check', 'staff', 'club', 'europ', 'area', 'display', 'premium', 'servic', 'provid', 'good', 'overal', 'servic', 'galleri', 'south', 'loung', 'busi', 'food', 'good', 'good', 'array', 'salad', 'pasta', 'bread', 'food', 'avail', 'festiv', 'turkey', 'particular', 'good', 'flight', 'delay', 'busi', 'tuesday', 'even', 'board', 'process', 'stress', 'lack', 'queue', 'manag', 'major', 'passeng', 'congreg', 'gate', 'despit', 'group', 'board', 'took', 'board', 'passeng', 'flight', 'good', 'good', 'club', 'meal', 'servic', 'servic', 'good', 'away', 'champagn', 'board', 'loung', 'menu', 'hand', 'prior', 'departur', 'good', 'food', 'offer', 'length', 'flight', 'food', 'good', 'beef', 'cheek', 'serv', 'veget', 'chosen', 'excel', 'cater', 'chang', 'seat', 'standard', 'economi', 'seat', 'tabl', 'middl', 'seat', 'club', 'europ', 'standard', 'practic', 'seat', 'comfort', 'limit', 'room', 'cabin', 'clean', 'present', 'tidi', 'issu', 'featur', 'contain', 'good', 'land', 'page', 'flight', 'inform', 'access', 'speedbird', 'cafe', 'menu', 'access', 'good', 'price', 'strategi', 'wither', 'messag', 'stream', 'overal', 'airlin', 'good', 'world', 'player', 'clear', 'improv', 'need', 'ground', 'staff', 'cabin', 'manag', 'good', 'crew', 'club', 'display', 'expect', 'behaviour', 'busi', 'class', 'kept', 'touch', 'face', 'make', 'contact', 'passeng', 'uninterest', 'like', 'want'], ['stupid', 'tri', 'year', 'paid', 'wife', 'barbado', 'thought', 'busi', 'class', 'layout', 'rival', 'virgin', 'atlant', 'rout', 'month', 'announc', 'switch', 'plane', 'antiqu', 'push', 'pull', 'layout', 'late', 'reorganis', 'holiday', 'logist', 'sure', 'suffer', 'appal', 'uncomfort', 'flight', 'room', 'seat', 'worn', 'fit', 'ined', 'food', 'sur'], ['glasgow', 'london', 'delay', 'hour', 'wife', 'accept', 'flight', 'late', 'delay', 'rare', 'time', 'long', 'accept', 'disappoint', 'plan', 'ahead', 'high', 'risk', 'delay', 'connect', 'airlin', 'oneworld', 'extrem', 'riski'], ['tri', 'check', 'onlin', 'offer', 'upgrad', 'premium', 'economi', 'decid', 'websit', 'wouldn', 'allow', 'choos', 'seat', 'check', 'phone', 'happi', 'pick', 'quick', 'told', 'chang', 'book', 'check', 'airport', 'turn', 'hour', 'flight', 'told', 'adjac', 'aisl', 'seat', 'economi', 'switch', 'middl', 'seat', 'premium', 'check', 'agent', 'help', 'couldn', 'suggest', 'desk', 'airsid', 'desk', 'understaf', 'reach', 'line', 'told', 'board', 'gate', 'gate', 'final', 'manag', 'aisl', 'seat', 'window', 'sit', 'go', 'flight', 'surpris', 'good', 'decent', 'food', 'good', 'servic', 'flew', 'economi', 'wasn', 'flight', 'attend', 'friend', 'enthusiast', 'emma', 'daniel', 'look', 'better', 'obvious', 'learn', 'lesson', 'late', 'upgrad', 'good'], ['flew', 'pragu', 'excel', 'servic', 'attent', 'staff', 'drink', 'servic', 'meal', 'servic', 'choic', 'meal', 'later', 'second', 'drink', 'servic', 'realiti', 'excel', 'flight', 'coupl', 'day', 'later', 'flew', 'pragu', 'board', 'total', 'chao', 'onboard', 'servic', 'call', 'servic', 'curtain', 'busi', 'economi', 'class', 'later', 'caus', 'peopl', 'endless', 'pass', 'toilet', 'meal', 'servic', 'unprofession', 'row', 'took', 'guy', 'near', 'min', 'serv', 'constant', 'interrupt', 'peopl', 'pass', 'toilet', 'time', 'know', 'meal', 'choic', 'bother', 'talk', 'serv', 'afternoon', 'consist', 'canap', 'like', 'bite', 'drink', 'bother', 'serv', 'scone', 'hard', 'min', 'plate', 'taken', 'away', 'ask', 'scone', 'came', 'surpris', 'serv', 'ordin', 'crew', 'crew', 'busi', 'chat', 'galley', 'realiti', 'servic', 'wors', 'premium', 'price']]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preview the processed documents\n",
    "'''\n",
    "print(processed_docs[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a dictionary of the words which appear in the entire corpus\n",
    "'''\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.keys()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 accept\n",
      "1 advic\n",
      "2 airport\n",
      "3 appal\n",
      "4 book\n",
      "5 cancel\n",
      "6 check\n",
      "7 close\n",
      "8 condit\n",
      "9 despit\n",
      "10 downgrad\n"
     ]
    }
   ],
   "source": [
    "#print a few words in the dictionary\n",
    "count = 0\n",
    "for k,v in dictionary.iteritems():\n",
    "    print(k,v)\n",
    "    count = count + 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gensim filter extremes\n",
    "\n",
    "    Remove or filter the words that appear less than nobelow\n",
    "    Remove or filter the words that apepar more than noabove (fraction)\n",
    "    After the above two steps keep only the n most frequent tokens or keep all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5,no_above=0.1,keep_n=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Gensim doc2bow\n",
    "\n",
    "    Create a bag of words for each document ie for each document we create a dictionary reporting how many words and how many times those words appear\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(23, 1), (288, 1), (298, 1), (299, 1), (300, 1)],\n",
       " [(71, 1),\n",
       "  (257, 1),\n",
       "  (258, 1),\n",
       "  (301, 1),\n",
       "  (302, 1),\n",
       "  (303, 1),\n",
       "  (304, 1),\n",
       "  (305, 1),\n",
       "  (306, 1),\n",
       "  (307, 1),\n",
       "  (308, 1)],\n",
       " [(50, 3),\n",
       "  (64, 1),\n",
       "  (66, 3),\n",
       "  (67, 1),\n",
       "  (103, 1),\n",
       "  (111, 1),\n",
       "  (116, 1),\n",
       "  (119, 1),\n",
       "  (123, 1),\n",
       "  (173, 1),\n",
       "  (212, 1),\n",
       "  (223, 1),\n",
       "  (266, 1),\n",
       "  (309, 1),\n",
       "  (310, 1),\n",
       "  (311, 1),\n",
       "  (312, 2),\n",
       "  (313, 1),\n",
       "  (314, 1),\n",
       "  (315, 1),\n",
       "  (316, 1),\n",
       "  (317, 1),\n",
       "  (318, 1),\n",
       "  (319, 1),\n",
       "  (320, 1),\n",
       "  (321, 1),\n",
       "  (322, 2),\n",
       "  (323, 4),\n",
       "  (324, 1),\n",
       "  (325, 2),\n",
       "  (326, 1),\n",
       "  (327, 1),\n",
       "  (328, 1),\n",
       "  (329, 1),\n",
       "  (330, 1),\n",
       "  (331, 1),\n",
       "  (332, 1),\n",
       "  (333, 5)],\n",
       " [(16, 1),\n",
       "  (91, 1),\n",
       "  (125, 1),\n",
       "  (144, 2),\n",
       "  (165, 1),\n",
       "  (182, 2),\n",
       "  (191, 1),\n",
       "  (203, 1),\n",
       "  (244, 1),\n",
       "  (290, 3),\n",
       "  (334, 1),\n",
       "  (335, 1),\n",
       "  (336, 2),\n",
       "  (337, 1),\n",
       "  (338, 1),\n",
       "  (339, 1),\n",
       "  (340, 1),\n",
       "  (341, 1),\n",
       "  (342, 1),\n",
       "  (343, 1),\n",
       "  (344, 1),\n",
       "  (345, 1),\n",
       "  (346, 1)],\n",
       " [(68, 1),\n",
       "  (271, 2),\n",
       "  (273, 1),\n",
       "  (347, 1),\n",
       "  (348, 1),\n",
       "  (349, 1),\n",
       "  (350, 2),\n",
       "  (351, 1)],\n",
       " [(18, 1),\n",
       "  (71, 1),\n",
       "  (122, 1),\n",
       "  (238, 1),\n",
       "  (257, 1),\n",
       "  (303, 1),\n",
       "  (352, 1),\n",
       "  (353, 1),\n",
       "  (354, 1),\n",
       "  (355, 1),\n",
       "  (356, 1),\n",
       "  (357, 1),\n",
       "  (358, 1),\n",
       "  (359, 1),\n",
       "  (360, 1),\n",
       "  (361, 1),\n",
       "  (362, 1),\n",
       "  (363, 1)],\n",
       " [(17, 1),\n",
       "  (29, 1),\n",
       "  (48, 2),\n",
       "  (49, 2),\n",
       "  (55, 2),\n",
       "  (66, 1),\n",
       "  (71, 1),\n",
       "  (80, 2),\n",
       "  (151, 2),\n",
       "  (155, 1),\n",
       "  (168, 1),\n",
       "  (180, 1),\n",
       "  (193, 2),\n",
       "  (205, 2),\n",
       "  (252, 1),\n",
       "  (271, 1),\n",
       "  (295, 1),\n",
       "  (296, 1),\n",
       "  (316, 1),\n",
       "  (364, 1),\n",
       "  (365, 2),\n",
       "  (366, 1),\n",
       "  (367, 1),\n",
       "  (368, 1),\n",
       "  (369, 1),\n",
       "  (370, 1),\n",
       "  (371, 1),\n",
       "  (372, 2),\n",
       "  (373, 1),\n",
       "  (374, 2),\n",
       "  (375, 2),\n",
       "  (376, 2),\n",
       "  (377, 1),\n",
       "  (378, 1),\n",
       "  (379, 1),\n",
       "  (380, 1),\n",
       "  (381, 1)],\n",
       " [(4, 1),\n",
       "  (27, 1),\n",
       "  (58, 1),\n",
       "  (61, 1),\n",
       "  (65, 1),\n",
       "  (67, 2),\n",
       "  (70, 1),\n",
       "  (73, 1),\n",
       "  (79, 1),\n",
       "  (95, 1),\n",
       "  (126, 1),\n",
       "  (146, 3),\n",
       "  (150, 2),\n",
       "  (175, 1),\n",
       "  (191, 1),\n",
       "  (203, 2),\n",
       "  (213, 1),\n",
       "  (219, 1),\n",
       "  (242, 1),\n",
       "  (265, 2),\n",
       "  (271, 1),\n",
       "  (302, 1),\n",
       "  (381, 1),\n",
       "  (382, 1),\n",
       "  (383, 1),\n",
       "  (384, 1),\n",
       "  (385, 1),\n",
       "  (386, 1),\n",
       "  (387, 1),\n",
       "  (388, 1),\n",
       "  (389, 1),\n",
       "  (390, 1),\n",
       "  (391, 1),\n",
       "  (392, 1),\n",
       "  (393, 1),\n",
       "  (394, 1),\n",
       "  (395, 1),\n",
       "  (396, 1),\n",
       "  (397, 1),\n",
       "  (398, 1),\n",
       "  (399, 1),\n",
       "  (400, 1),\n",
       "  (401, 1),\n",
       "  (402, 1),\n",
       "  (403, 1),\n",
       "  (404, 2),\n",
       "  (405, 1),\n",
       "  (406, 1),\n",
       "  (407, 1),\n",
       "  (408, 1),\n",
       "  (409, 1),\n",
       "  (410, 1),\n",
       "  (411, 1),\n",
       "  (412, 1),\n",
       "  (413, 1),\n",
       "  (414, 1),\n",
       "  (415, 1)],\n",
       " [(180, 1),\n",
       "  (203, 1),\n",
       "  (240, 1),\n",
       "  (306, 1),\n",
       "  (325, 1),\n",
       "  (416, 1),\n",
       "  (417, 1),\n",
       "  (418, 1),\n",
       "  (419, 2)],\n",
       " [(26, 1),\n",
       "  (67, 1),\n",
       "  (70, 2),\n",
       "  (213, 2),\n",
       "  (253, 1),\n",
       "  (306, 1),\n",
       "  (311, 2),\n",
       "  (336, 1),\n",
       "  (416, 1),\n",
       "  (420, 1),\n",
       "  (421, 1),\n",
       "  (422, 1),\n",
       "  (423, 1),\n",
       "  (424, 1),\n",
       "  (425, 1),\n",
       "  (426, 1),\n",
       "  (427, 1),\n",
       "  (428, 2),\n",
       "  (429, 1),\n",
       "  (430, 1),\n",
       "  (431, 2),\n",
       "  (432, 1),\n",
       "  (433, 1),\n",
       "  (434, 1),\n",
       "  (435, 1),\n",
       "  (436, 1),\n",
       "  (437, 1),\n",
       "  (438, 1),\n",
       "  (439, 1),\n",
       "  (440, 1),\n",
       "  (441, 1),\n",
       "  (442, 1),\n",
       "  (443, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London Heathrow to Cape Town with British Airways. Paid extra to book 2 particular seats. Both were broken. I had to sit upright for 11+hours and my husband had to sit partially reclined to eat his meals. I had veggie meal which was served half hour before everyone else - no drink until everyone else served. Choice of meal for husband was chicken curry or chicken curry. He doesn't like curry so it was whispered to him that he could have a veggie meal. Same at breakfast - served half hour early so had to eat alone. When other breakfasts came out, guess what no meat. On complaining to BA Customer Services, told could choose another meal if we had gone on line and paid extra. Been offered the insulting and derisory compensation of £20 off our next flight (as if there will be a next flight) - not even offered a refund of payment we made for booking broken seat. Both airline and customer service disgraceful.\n"
     ]
    }
   ],
   "source": [
    "random = np.random.randint(1,len(neg_reviews))\n",
    "print(neg_reviews['reviews'][random])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_num = random\n",
    "bow_doc_x = bow_corpus[random]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 36 refund appears 1 times\n",
      "Word 49 compens appears 1 times\n",
      "Word 59 gone appears 1 times\n",
      "Word 64 line appears 1 times\n",
      "Word 94 came appears 1 times\n",
      "Word 108 half appears 2 times\n",
      "Word 215 particular appears 1 times\n",
      "Word 256 choos appears 1 times\n",
      "Word 359 reclin appears 1 times\n",
      "Word 387 cape appears 1 times\n",
      "Word 411 town appears 1 times\n",
      "Word 445 disgrac appears 1 times\n",
      "Word 573 broken appears 2 times\n",
      "Word 747 complain appears 1 times\n",
      "Word 749 husband appears 2 times\n",
      "Word 938 guess appears 1 times\n",
      "Word 1036 earli appears 1 times\n",
      "Word 1062 payment appears 1 times\n",
      "Word 1185 chicken appears 2 times\n",
      "Word 1524 insult appears 1 times\n",
      "Word 1737 meat appears 1 times\n",
      "Word 1793 upright appears 1 times\n",
      "Word 1856 curri appears 3 times\n",
      "Word 1879 veggi appears 2 times\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bow_doc_x)):\n",
    "    print(f'Word {bow_doc_x[i][0]} {dictionary[bow_doc_x[i][0]]} appears {bow_doc_x[i][1]} times')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running LDA using Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the gensim LDA model and generate 12 topics from the corpus\n",
    "seed = 41\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,num_topics=12,id2word=dictionary,passes=10,workers=2,\n",
    "                                      random_state=seed,minimum_probability=0.05,alpha='symmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"bag\" + 0.006*\"fast\" + 0.006*\"haul\" + 0.005*\"attend\" + 0.005*\"connect\" + 0.005*\"issu\" + 0.004*\"final\" + 0.004*\"dinner\" + 0.004*\"hotel\" + 0.004*\"dublin\" + 0.004*\"night\" + 0.004*\"option\" + 0.004*\"aw\" + 0.004*\"sleep\" + 0.004*\"product\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.007*\"think\" + 0.007*\"nice\" + 0.007*\"main\" + 0.006*\"lunch\" + 0.006*\"chicken\" + 0.006*\"option\" + 0.006*\"flown\" + 0.006*\"money\" + 0.005*\"go\" + 0.005*\"rout\" + 0.005*\"sandwich\" + 0.005*\"wife\" + 0.005*\"beef\" + 0.005*\"cut\" + 0.005*\"start\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.031*\"refund\" + 0.020*\"voucher\" + 0.017*\"email\" + 0.013*\"phone\" + 0.012*\"week\" + 0.011*\"onlin\" + 0.010*\"receiv\" + 0.009*\"month\" + 0.008*\"sent\" + 0.008*\"issu\" + 0.008*\"money\" + 0.007*\"contact\" + 0.007*\"number\" + 0.007*\"websit\" + 0.007*\"rebook\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.011*\"baggag\" + 0.011*\"connect\" + 0.008*\"miss\" + 0.008*\"close\" + 0.007*\"hotel\" + 0.007*\"point\" + 0.006*\"desk\" + 0.006*\"compens\" + 0.006*\"bag\" + 0.006*\"result\" + 0.005*\"secur\" + 0.005*\"understand\" + 0.005*\"walk\" + 0.005*\"compani\" + 0.005*\"earlier\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"qualiti\" + 0.008*\"rude\" + 0.008*\"problem\" + 0.007*\"ground\" + 0.007*\"movi\" + 0.007*\"suitcas\" + 0.007*\"montreal\" + 0.006*\"select\" + 0.005*\"reason\" + 0.005*\"usual\" + 0.005*\"wine\" + 0.005*\"absolut\" + 0.005*\"averag\" + 0.005*\"product\" + 0.004*\"instead\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.008*\"care\" + 0.007*\"addit\" + 0.006*\"reason\" + 0.006*\"uncomfort\" + 0.006*\"select\" + 0.006*\"snack\" + 0.005*\"terribl\" + 0.004*\"rout\" + 0.004*\"aw\" + 0.004*\"termin\" + 0.004*\"miss\" + 0.004*\"option\" + 0.004*\"come\" + 0.004*\"go\" + 0.004*\"problem\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.010*\"problem\" + 0.009*\"agent\" + 0.009*\"phone\" + 0.008*\"receiv\" + 0.008*\"later\" + 0.007*\"respons\" + 0.006*\"tell\" + 0.006*\"say\" + 0.006*\"number\" + 0.005*\"select\" + 0.005*\"advis\" + 0.005*\"short\" + 0.005*\"haul\" + 0.005*\"onlin\" + 0.005*\"thing\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.010*\"miss\" + 0.010*\"connect\" + 0.008*\"compens\" + 0.008*\"bag\" + 0.008*\"claim\" + 0.007*\"inform\" + 0.007*\"later\" + 0.007*\"line\" + 0.006*\"case\" + 0.006*\"room\" + 0.006*\"refund\" + 0.005*\"went\" + 0.005*\"gave\" + 0.005*\"final\" + 0.005*\"couldn\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.007*\"toilet\" + 0.007*\"queue\" + 0.006*\"great\" + 0.006*\"aisl\" + 0.006*\"select\" + 0.006*\"termin\" + 0.006*\"standard\" + 0.005*\"departur\" + 0.005*\"window\" + 0.005*\"start\" + 0.005*\"littl\" + 0.005*\"clean\" + 0.005*\"person\" + 0.005*\"secur\" + 0.004*\"overal\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.010*\"carrier\" + 0.009*\"member\" + 0.008*\"budget\" + 0.007*\"haul\" + 0.007*\"point\" + 0.007*\"allow\" + 0.006*\"short\" + 0.006*\"know\" + 0.006*\"fare\" + 0.005*\"termin\" + 0.005*\"attend\" + 0.005*\"connect\" + 0.005*\"lost\" + 0.005*\"feel\" + 0.005*\"free\"\n",
      "\n",
      "\n",
      "Topic: 10 \n",
      "Words: 0.014*\"rout\" + 0.010*\"easyjet\" + 0.009*\"worst\" + 0.007*\"haul\" + 0.006*\"ryanair\" + 0.005*\"sandwich\" + 0.005*\"complaint\" + 0.005*\"uncomfort\" + 0.005*\"product\" + 0.005*\"amsterdam\" + 0.005*\"suitcas\" + 0.005*\"carrier\" + 0.005*\"rude\" + 0.005*\"budget\" + 0.005*\"pack\"\n",
      "\n",
      "\n",
      "Topic: 11 \n",
      "Words: 0.007*\"rout\" + 0.006*\"option\" + 0.006*\"toilet\" + 0.005*\"card\" + 0.005*\"snack\" + 0.005*\"free\" + 0.005*\"come\" + 0.004*\"clear\" + 0.004*\"point\" + 0.004*\"sleep\" + 0.004*\"issu\" + 0.004*\"space\" + 0.004*\"queue\" + 0.004*\"worst\" + 0.004*\"carrier\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, explore each word and its relative weight in the topic\n",
    "'''\n",
    "\n",
    "for idx,topic in lda_model.print_topics(-1,num_words=15):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score: 0.2643318667918662\n"
     ]
    }
   ],
   "source": [
    "#import Coherence model from gensim\n",
    "from gensim.models import CoherenceModel\n",
    "#compute coherence score\n",
    "lda_model_coherence = CoherenceModel(model=lda_model,texts=processed_docs,dictionary=dictionary,\n",
    "                                    coherence='c_v')\n",
    "coherence_lda = lda_model_coherence.get_coherence()\n",
    "print('\\nCoherence Score:',coherence_lda)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a helper function\n",
    "def compute_coherence_score(corpus,dictionary,k,a):\n",
    "    #instantiate the model instance based on k,a and b\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=dictionary,num_topics=k,alpha=a,\n",
    "                                           passes=10,\n",
    "                                           random_state=seed)\n",
    "    lda_model_coherence = CoherenceModel(model=lda_model,texts=processed_docs,dictionary=dictionary,coherence='c_v')\n",
    "    return lda_model_coherence.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Model with alpha = symmetric\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "Coherence score with 5 topics is 0.34536201276023315\n",
      "\n",
      "\n",
      "Coherence score with 6 topics is 0.32535912395126915\n",
      "\n",
      "\n",
      "Coherence score with 7 topics is 0.3206426167084266\n",
      "\n",
      "\n",
      "Coherence score with 8 topics is 0.3191085079710594\n",
      "\n",
      "\n",
      "Coherence score with 9 topics is 0.28833318417347553\n",
      "\n",
      "\n",
      "Coherence Model with alpha = asymmetric\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "Coherence score with 5 topics is 0.3569745994597604\n",
      "\n",
      "\n",
      "Coherence score with 6 topics is 0.31517034073823563\n",
      "\n",
      "\n",
      "Coherence score with 7 topics is 0.3392142219267681\n",
      "\n",
      "\n",
      "Coherence score with 8 topics is 0.2871019837116862\n",
      "\n",
      "\n",
      "Coherence score with 9 topics is 0.2996681257915492\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#search for the best alpha and the number of topics --> one with the highest coherence score will be the best hyperparameter\n",
    "alpha =['symmetric','asymmetric']\n",
    "\n",
    "for x in alpha:\n",
    "    print('Coherence Model with alpha = {}'.format(x))\n",
    "    print('-------------------------------------------')\n",
    "    print('\\n')\n",
    "    for i in range(5,10):\n",
    "        score = compute_coherence_score(corpus=bow_corpus,dictionary=dictionary,k=i,a=x)        \n",
    "        print(f'Coherence score with {i} topics is {score}')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Though the model with alpha='asymmetric' and with 12 topics returned the highest coherence score. For the business use and simplicity, max topics of 7\n",
    "and alpha='asymmetric' as the final model. This also returned a comparable coherence score of 0.6262\n",
    "'''\n",
    "lda_model_final = gensim.models.LdaMulticore(corpus=bow_corpus,num_topics=7,id2word=dictionary,passes=10,workers=2,alpha='asymmetric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.007*\"haul\" + 0.006*\"toilet\" + 0.006*\"short\" + 0.006*\"carrier\" + '\n",
      "  '0.006*\"snack\" + 0.005*\"littl\" + 0.005*\"select\" + 0.005*\"free\" + '\n",
      "  '0.005*\"europ\" + 0.005*\"room\"'),\n",
      " (1,\n",
      "  '0.005*\"wine\" + 0.005*\"sleep\" + 0.005*\"secur\" + 0.005*\"came\" + 0.005*\"went\" '\n",
      "  '+ 0.005*\"hard\" + 0.004*\"tray\" + 0.004*\"clear\" + 0.004*\"member\" + '\n",
      "  '0.004*\"option\"'),\n",
      " (2,\n",
      "  '0.009*\"rout\" + 0.009*\"week\" + 0.008*\"worst\" + 0.006*\"compani\" + '\n",
      "  '0.006*\"york\" + 0.006*\"famili\" + 0.005*\"month\" + 0.005*\"home\" + '\n",
      "  '0.005*\"complaint\" + 0.005*\"problem\"'),\n",
      " (3,\n",
      "  '0.008*\"option\" + 0.007*\"attend\" + 0.007*\"nice\" + 0.006*\"upgrad\" + '\n",
      "  '0.006*\"menu\" + 0.006*\"think\" + 0.005*\"aisl\" + 0.005*\"rout\" + 0.005*\"budget\" '\n",
      "  '+ 0.004*\"start\"'),\n",
      " (4,\n",
      "  '0.008*\"termin\" + 0.008*\"queue\" + 0.007*\"departur\" + 0.006*\"inform\" + '\n",
      "  '0.006*\"point\" + 0.006*\"voucher\" + 0.006*\"desk\" + 0.006*\"allow\" + '\n",
      "  '0.006*\"drop\" + 0.006*\"amsterdam\"'),\n",
      " (5,\n",
      "  '0.024*\"refund\" + 0.012*\"miss\" + 0.012*\"email\" + 0.012*\"connect\" + '\n",
      "  '0.011*\"phone\" + 0.010*\"issu\" + 0.010*\"receiv\" + 0.009*\"inform\" + '\n",
      "  '0.009*\"claim\" + 0.009*\"rebook\"'),\n",
      " (6,\n",
      "  '0.011*\"onlin\" + 0.011*\"reason\" + 0.006*\"start\" + 0.006*\"min\" + 0.006*\"desk\" '\n",
      "  '+ 0.006*\"request\" + 0.005*\"choos\" + 0.005*\"abl\" + 0.005*\"number\" + '\n",
      "  '0.005*\"option\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 7 topics\n",
    "pprint(lda_model_final.print_topics())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.interfaces.TransformedCorpus at 0x25b78fafcd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_lda = lda_model_final[bow_corpus]\n",
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyLDAvis' has no attribute 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\DS_AI\\Digital Futures\\Projects\\extras\\ba\\topic_model.ipynb Cell 30\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DS_AI/Digital%20Futures/Projects/extras/ba/topic_model.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# visulaise the topics\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DS_AI/Digital%20Futures/Projects/extras/ba/topic_model.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pyLDAvis\u001b[39m.\u001b[39menable_notebook()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/DS_AI/Digital%20Futures/Projects/extras/ba/topic_model.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m vis\u001b[39m=\u001b[39mpyLDAvis\u001b[39m.\u001b[39;49mgensim\u001b[39m.\u001b[39mprepare(lda_model,corpus,id2word)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/DS_AI/Digital%20Futures/Projects/extras/ba/topic_model.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m vis\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pyLDAvis' has no attribute 'gensim'"
     ]
    }
   ],
   "source": [
    "# visulaise the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis=pyLDAvis.gensim.prepare(lda_model,corpus,id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = decomposition.LatentDirichletAllocation(n_components=10, *,\n",
    " doc_topic_prior=None, topic_word_prior=None, learning_method='batch', learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1,\n",
    " mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None, verbose=0, random_state=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c2d5e71f59c2e4535768c8af4272ebdceb904fbebd50d6958e23c04b61c10198"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
